---
layout: post
title:  "Hadoop: The Definitive Guide (2)"
categories: Hadoop
tags:  Hadoop BigData MapReduce
---

* content
{:toc}


## MapReduce

A MapReduce job is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information. Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in the cluster.

Hadoop divides the input to a MapReduce job into fixed-size pieces called input splits. Hadoop creates one map task for each split. For most jobs, a good split size tends to be the size of an HDFS block (it is the largest size of input that can be guaranteed to be stored on a single node, to have the data locality optimization).

<img src="/images/posts_images/Hadoop/hddg_0204.png" alt="Procedure Graph" style="width: 800px;"/>

Also, Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s output forms the input to the reduce function.


## Developing a MapReduce application

### Steps
0. set up and configure the development environment
1. write your map and reduce functions, ideally with unit tests to make sure they do what you expect
2. write a driver program to run a job, which can run from your IDE using a small subset of the data to check that it is working
3. use your IDE’s debugger to find the source of the problem, expand your unit tests to cover this case and improve your mapper or reducer as appropriate to handle such input correctly
4. unleash it on a cluster
5. expand your tests and alter your mapper or reducer to handle the new cases
6. tune first by running through some standard checks for making MapReduce programs faster and then by doing task profiling

### Hadoop configuration
An instance of the Configuration class (found in the org.apache.hadoop.conf package) represents a collection of configuration properties and their values. It can be passed into class: `driver.setConf(conf);`

For building MapReduce jobs, you only need to have the hadoop-client dependency, which contains all the Hadoop client-side classes needed to interact with HDFS and MapReduce. For running unit tests, we use junit, and for writing MapReduce tests, we use mrunit. The hadoop-minicluster library contains the “mini-” clusters that are useful for testing with Hadoop clusters running in a single JVM.

It’s more convenient to implement the Tool interface and run your application with the ToolRunner, which uses GenericOptionsParser internally:

```java
public interface Tool extends Configurable {
  int run(String [] args) throws Exception;
}


public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new MaxTemperatureDriver(), args);
    System.exit(exitCode);
  }
```

ToolRunner supports options including `-D property=value`, `-conf filename`, ...

### MRUnit

Test mapper and reducer:
```java
public void testMap() throws IOException, InterruptedException {
    // ...
    new MapDriver<LongWritable, Text, Text, IntWritable>()
          .withMapper(new MaxTemperatureMapper())
          .withInput(new LongWritable(0), value)
          .withOutput(new Text("1950"), new IntWritable(-11))
          .runTest();
    // ...
}

public void testMap() throws IOException, InterruptedException {
    // ...
    new ReduceDriver<Text, IntWritable, Text, IntWritable>()
          .withReducer(new MaxTemperatureReducer())
          .withInput(new Text("1950"), Arrays.asList(new IntWritable(10), new IntWritable(5)))
          .withOutput(new Text("1950"), new IntWritable(10))
          .runTest();
    // ...
}
```

Test driver:
```java
public class TestDriver extends Configured implements Tool {

  @Override
  public int run(String[] args) throws Exception {
    if (args.length != 2) {
      System.err.printf("Usage: %s [generic options] <input> <output>\n",
              getClass().getSimpleName());
      ToolRunner.printGenericCommandUsage(System.err);
      return -1;
    }

    Job job = new Job(getConf(), "Max temperature");
    job.setJarByClass(getClass());

    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.setMapperClass(TestMapper.class);
    job.setCombinerClass(TestReducer.class);
    job.setReducerClass(TestReducer.class);

    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    return job.waitForCompletion(true) ? 0 : 1;
  }

  public static void main(String[] args) throws Exception {
    int exitCode = ToolRunner.run(new TestDriver(), args);
    System.exit(exitCode);
  }
}
```

### Running on a cluster

1. Packaging a job
  - In a distributed setting, things are a little more complex. For a start, a job’s classes must be packaged into a job JAR file to send to the cluster. Hadoop will find the job JAR automatically by searching for the JAR on the driver’s classpath that contains the class set in the setJarByClass() method (on JobConf or Job). Alternatively, if you want to set an explicit JAR file by its file path, you can use the setJar() method. (The JAR file path may be local or an HDFS file path.)
  - Classpaths: job JAR file, JAR files contained in the lib directory of the job JAR file and the classes directory, files added to the distributed cache using the -libjars option or the addFileToClassPath() method
  - On the client side, you can force Hadoop to put the user classpath first in the search order by setting the HADOOP_USER_CLASSPATH_FIRST environment variable to true. For the task classpath, you can set mapreduce.job.user.classpath.first to true.

2. Launching a Job
`hadoop jar hadoop-examples.jar v2.MaxTemperatureDriver -conf conf/hadoop-cluster.xml input/ncdc/all max-temp`
The waitForCompletion() method on Job launches the job and polls for progress, writing a line summarizing the map and reduce’s progress whenever either changes.

3. Get results
Another way of retrieving the output if it is small is to use the -cat option to print the output files to the console:
`% hadoop fs -cat max-temp/*`

4. Debug a job
  - Print statements
  - Tasks and task attempts pages
  - Unit test to handle malformed data
  - Hadoop logs


### Workflows

- JobControl
- Oozie
    Apache Oozie is a system for running workflows of dependent jobs. It is composed of two main parts: a workflow engine that stores and runs workflows composed of different types of Hadoop jobs (MapReduce, Pig, Hive, and so on), and a coordinator engine that runs workflow jobs based on predefined schedules and data availability. Oozie has been designed to scale, and it can manage the timely execution of thousands of workflows in a Hadoop cluster, each composed of possibly dozens of constituent jobs.

