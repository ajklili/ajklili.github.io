---
layout: post
title:  "Airflow Deployment in AWS"
categories: technology
tags:  cloud Airflow
excerpt: "Data Pipelines with Apache Airflow"
---

* content
{:toc}

## Introduction
In the book "Data Pipelines with Apache Airflow", many concepts, mechanisms and usages of Airflow are introduced.

Here I would like to share two chapters, which I feel the ideas can be applied to other software and applications.

## Pipeline job best practices

1. Coding style
	- Developers in a team should follow the same style guides, e.g.
		- Python [PEP-8](https://peps.python.org/pep-0008/)
		- Python [Google](https://google.github.io/styleguide/pyguide.html)
	- Use checkers to force the style in merge request CI/CD
		- [Flake8](https://flake8.pycqa.org/en/latest/)
	- Use code formatters before code commits
		- [YAPF](https://github.com/google/yapf)
		- [Black](https://github.com/psf/black)
	- Set up some application-specific style conversions
	- Preferable to use factory pattern for better code reusability

2. Manage configurations
	- Use a central place to manage all changable parameters
	- Take care of credentials & secrets
	- Put lists of variables into YAML/JSON files

3. Design tasks
	- Group closely-related tasks together
	- Use version control and create new jobs for big changes
	- A single task should be designed to be:
		- idempotent (the task is rerunable, same results in source & destination)
		- deterministic (the task is rerunable, same output)
		- functional paradigms
		- limited resource requirement on local environment (CPU, filesystem)
	- Optimize the tasks:
	    - decouple light and heavy workload into different environments, connect them with async methods
		- try to split large dataset into smaller/incremental ones for higher efficiency
		- improve task speed by cache intermediate data
	- Always add monitoring and alerting
	

## Airflow deployment pattern evolution

Simple architecure:

<img src="/images/posts_images/Airflow/local0.png" style="width: 500px;"/>

\+ Use a single worker with multiple process:

<img src="/images/posts_images/Airflow/local1.png" style="width: 500px;"/>

++ Use a queue to manage multiple workers:

<img src="/images/posts_images/Airflow/local2.png" style="width: 500px;"/>

Cloud environment deployment:

<img src="/images/posts_images/Airflow/cloud.png" style="width: 500px;"/>

\+ Use AWS services:

- NAT gateway + ALB (load balancer): serving as public endpoints
- Eni (network interface): connect public & private subnets
- Fargate: application webserver
- RDS: metastore
- DFS: local shared storage
- S3: log storage & object/data storage
- Fargate: core engine (scheduler and workers)

<img src="/images/posts_images/Airflow/aws0.png" style="width: 500px;"/>

++ Use Lambda for CI/CD

<img src="/images/posts_images/Airflow/aws1.png" style="width: 500px;"/>

++ Use SQS (queue) to manage workers from scheduler
<img src="/images/posts_images/Airflow/aws2.png" style="width: 500px;"/>